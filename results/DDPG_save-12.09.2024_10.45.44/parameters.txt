PARAMETERS


initial_xyzs: [[4.5 3.5 0.2]]
random_initial_pos: False
ctrl_freq: 240
Target_pos: [2.5 2.  0.2]
episode_length: 30
Learning_rate: 0.0005
Learning_rate_decay: -0.005
Target_reward: 100000
Rew_distrav_fact: 0
Rew_disway_fact: 0.01
Rew_step_fact: 0
Rew_tardis_fact: 100
Rew_collision: -1000
Rew_terminated: 1000
eval_freq: 1
position: True
velocity: True
rpy: True
ang_v: True
prev_act: False
number_of_env: 16
Total_timesteps: 10000000
train_freq: 1
Reward_Function: (-self.Rew_distrav_fact*(np.linalg.norm(self.reward_state[0:2]-prev_state[0:2]))-self.Rew_disway_fact*(np.linalg.norm(self.TARGET_POS[0:2]-self.reward_state[0:2])**4)-self.Rew_step_fact*1 +self.Rew_tardis_fact*(prev_tar_dis-self.target_dis))
